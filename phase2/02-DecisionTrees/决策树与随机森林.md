# 决策树与随机森林

## 1. 决策树

### 1.1 基本流程

+ 决策树既可以做分类，又可以做回归。
+ 决策树的生成是一个递归过程

### 1.2 划分选择

+ 如何选择最优划分属性

#### 1.2.1 信息增益

**熵**

<img src="D:\Work\Learning\machineLearning\phase2\02-DecisionTrees\img\image-20200522003804016.png" alt="image-20200522003804016" style="zoom:50%;" />

熵值越小，纯度越高

**信息增益**

<img src="D:\Work\Learning\machineLearning\phase2\02-DecisionTrees\img\image-20200522003824176.png" alt="image-20200522003824176" style="zoom:50%;" />

信息增益越大，则意味着用属性a来进行划分所获得的纯度提升越大

**ID3**决策树学习算法，根据信息增益为准则来选择划分属性

#### 1.2.2 增益率

信息增益准则对可取值数目较多的属性有所偏好，增益率用来改善

增益率

<img src="D:\Work\Learning\machineLearning\phase2\02-DecisionTrees\img\image-20200522003903397.png" alt="image-20200522003903397" style="zoom:50%;" />

IV为属性a的固有值，一般数目越多，这个值会越大

<img src="D:\Work\Learning\machineLearning\phase2\02-DecisionTrees\img\image-20200522003903397.png" alt="image-20200522003903397" style="zoom:50%;" />

C4.5决策树学习算法，根据增益率来选择最优划分属性

#### 1.2.3 基尼指数

基尼值

<img src="D:\Work\Learning\machineLearning\phase2\02-DecisionTrees\img\image-20200522004005698.png" alt="image-20200522004005698" style="zoom:50%;" />

基尼值越小，数据集纯度越高

### 1.3 剪枝处理

